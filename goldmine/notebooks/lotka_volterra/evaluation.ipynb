{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lotka-Volterra inference tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import corner\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import collections\n",
    "\n",
    "from goldmine.inference.nde import MAFInference\n",
    "from goldmine.simulators.lotka_volterra import LotkaVolterra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000]\n",
    "\n",
    "methods = ['carl', 'maf','rascal', 'cascal', 'scandal', 'scandal', 'scandal_checkpoint']\n",
    "model_labels = ['', '_mog','', '', '_mog_largealpha', '_mog', '_mog']\n",
    "legend_labels = [r'LRT, ROLR$\\,$', r'MAF$\\,$',\n",
    "                 r'RASCAL, $\\alpha$=0.01', r'CASCAL, $\\alpha$=0.01', '', r'SCANDAL, $\\alpha$=0.01 (dashed: 0.1)', 'Checkpoints']\n",
    "\n",
    "linestyles = ['-', '-','-', '-', '--', '-', ':']\n",
    "colors      = ['darkgreen', 'C1','#CC002E','#be96ff','#0064b4','#0064b4', 'black']\n",
    "band_alphas = [0.15 for _ in colors]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_l_absolute = 8. * 0.1\n",
    "margin_r_absolute = 8. * 0.02\n",
    "margin_r_absolute_extra = 8. * 0.1\n",
    "margin_sep_absolute = 8. * 0.02\n",
    "margin_sep_small_absolute = 8. * 0.01\n",
    "margin_t_absolute = 8. * 0.02\n",
    "margin_t_absolute_extra = 8. * 0.12\n",
    "margin_b_absolute = 8. * 0.08\n",
    "\n",
    "\n",
    "def custom_figure(\n",
    "    n_panels=2,\n",
    "    width=8.,\n",
    "    panel_aspect_ratio=1.,\n",
    "    extra_top_space=False,\n",
    "    reduce_vertical_sep=False,\n",
    "):\n",
    "\n",
    "    if isinstance(n_panels, collections.Sequence):\n",
    "        n_panels_h, n_panels_v = n_panels\n",
    "    else:\n",
    "        n_panels_h = n_panels\n",
    "        n_panels_v = 1\n",
    "\n",
    "    # Determine top margin\n",
    "    _margin_t_absolute = (\n",
    "        margin_t_absolute_extra if extra_top_space else margin_t_absolute\n",
    "    )\n",
    "\n",
    "    # Calculate horizontal margins. Units: relative to width.\n",
    "    margin_l = margin_l_absolute / width\n",
    "    margin_r = margin_r_absolute / width\n",
    "    margin_l_subsequent = margin_l\n",
    "    if n_panels_h > 2:\n",
    "        margin_l_subsequent = margin_r\n",
    "    margin_sep = margin_sep_absolute / width\n",
    "    if n_panels_h > 2:\n",
    "        margin_sep = 0\n",
    "    margin_sep_total = margin_r + margin_sep + margin_l_subsequent\n",
    "    panel_width = (\n",
    "        1. - margin_l - margin_r - (n_panels_h - 1) * margin_sep_total\n",
    "    ) / n_panels_h\n",
    "\n",
    "    # Calculate wspace argument of subplots_adjust\n",
    "    wspace = margin_sep_total / panel_width\n",
    "\n",
    "    # Calculate absolute height\n",
    "    panel_height_absolute = panel_width * width / panel_aspect_ratio\n",
    "    height = (\n",
    "        n_panels_v * (panel_height_absolute + _margin_t_absolute + margin_b_absolute)\n",
    "        + (n_panels_v - 1) * margin_sep_absolute\n",
    "    )\n",
    "\n",
    "    # Calculate vertical margins. Units: relative to width.\n",
    "    panel_height = panel_height_absolute / height\n",
    "    margin_t = _margin_t_absolute / height\n",
    "    margin_b = margin_b_absolute / height\n",
    "    if reduce_vertical_sep:\n",
    "        margin_sep_total = margin_sep_small_absolute / height\n",
    "    else:\n",
    "        margin_sep_total = margin_t + margin_b + margin_sep_absolute / height\n",
    "\n",
    "    # Calculate wspace argument of subplots_adjust\n",
    "    hspace = margin_sep_total / panel_height\n",
    "\n",
    "    # New figure\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "\n",
    "    # Adjust margins\n",
    "    plt.subplots_adjust(\n",
    "        left=margin_l,\n",
    "        right=1. - margin_r,\n",
    "        bottom=margin_b,\n",
    "        top=1. - margin_t,\n",
    "        wspace=wspace,\n",
    "        hspace=hspace,\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_height(n_panels=2, width=8., panel_aspect_ratio=1., extra_top_space=False, extra_r_space=False):\n",
    "    \n",
    "    if isinstance(n_panels, collections.Sequence):\n",
    "        n_panels_h, n_panels_v = n_panels\n",
    "    else:\n",
    "        n_panels_h = n_panels\n",
    "        n_panels_v = 1\n",
    "        \n",
    "    # Determine top margin\n",
    "    _margin_t_absolute = margin_t_absolute_extra if extra_top_space else margin_t_absolute\n",
    "    _margin_r_absolute = margin_r_absolute_extra if extra_r_space else margin_r_absolute\n",
    "    \n",
    "    # Calculate horizontal margins. Units: relative to width.\n",
    "    margin_l = margin_l_absolute / width\n",
    "    margin_r = _margin_r_absolute / width\n",
    "    margin_l_subsequent = margin_l\n",
    "    if n_panels_h > 2:\n",
    "        margin_l_subsequent = margin_r\n",
    "    margin_sep = margin_sep_absolute / width\n",
    "    if n_panels_h > 2:\n",
    "        margin_sep = 0\n",
    "    margin_sep_total = margin_r + margin_sep + margin_l_subsequent\n",
    "    panel_width = (1. - margin_l - margin_r - (n_panels_h - 1)*margin_sep_total) / n_panels_h\n",
    "    \n",
    "    # Calculate wspace argument of subplots_adjust\n",
    "    wspace = margin_sep_total / panel_width\n",
    "    \n",
    "    # Calculate absolute height\n",
    "    panel_height_absolute = panel_width * width / panel_aspect_ratio\n",
    "    height = n_panels_v * (panel_height_absolute + _margin_t_absolute + margin_b_absolute) + (n_panels_v - 1) * margin_sep_absolute\n",
    "    \n",
    "    # Calculate horizontal margins. Units: relative to width.\n",
    "    panel_height = panel_height_absolute / height\n",
    "    margin_t = _margin_t_absolute / height\n",
    "    margin_b = margin_b_absolute / height\n",
    "    margin_sep_total = (margin_t + margin_b + margin_sep_absolute / height)\n",
    "    \n",
    "    # Calculate wspace argument of subplots_adjust\n",
    "    hspace = margin_sep_total / panel_height\n",
    "    \n",
    "    # Return height\n",
    "    return height\n",
    "\n",
    "\n",
    "def adjust_margins(n_panels=2, width=8., panel_aspect_ratio=1., extra_top_space=False, extra_r_space=False):\n",
    "    \n",
    "    if isinstance(n_panels, collections.Sequence):\n",
    "        n_panels_h, n_panels_v = n_panels\n",
    "    else:\n",
    "        n_panels_h = n_panels\n",
    "        n_panels_v = 1\n",
    "        \n",
    "    # Determine top margin\n",
    "    _margin_t_absolute = margin_t_absolute_extra if extra_top_space else margin_t_absolute\n",
    "    _margin_r_absolute = margin_r_absolute_extra if extra_r_space else margin_r_absolute\n",
    "    \n",
    "    # Calculate horizontal margins. Units: relative to width.\n",
    "    margin_l = margin_l_absolute / width\n",
    "    margin_r = _margin_r_absolute / width\n",
    "    margin_l_subsequent = margin_l\n",
    "    if n_panels_h > 2:\n",
    "        margin_l_subsequent = margin_r\n",
    "    margin_sep = margin_sep_absolute / width\n",
    "    if n_panels_h > 2:\n",
    "        margin_sep = 0\n",
    "    margin_sep_total = margin_r + margin_sep + margin_l_subsequent\n",
    "    panel_width = (1. - margin_l - margin_r - (n_panels_h - 1)*margin_sep_total) / n_panels_h\n",
    "    \n",
    "    # Calculate wspace argument of subplots_adjust\n",
    "    wspace = margin_sep_total / panel_width\n",
    "    \n",
    "    # Calculate absolute height\n",
    "    panel_height_absolute = panel_width * width / panel_aspect_ratio\n",
    "    height = n_panels_v * (panel_height_absolute + _margin_t_absolute + margin_b_absolute) + (n_panels_v - 1) * margin_sep_absolute\n",
    "    \n",
    "    # Calculate horizontal margins. Units: relative to width.\n",
    "    panel_height = panel_height_absolute / height\n",
    "    margin_t = _margin_t_absolute / height\n",
    "    margin_b = margin_b_absolute / height\n",
    "    margin_sep_total = (margin_t + margin_b + margin_sep_absolute / height)\n",
    "    \n",
    "    # Calculate wspace argument of subplots_adjust\n",
    "    hspace = margin_sep_total / panel_height\n",
    "    \n",
    "    # Set margins\n",
    "    plt.subplots_adjust(left = margin_l,\n",
    "                        right = 1. - margin_r,\n",
    "                        bottom = margin_b,\n",
    "                        top = 1. - margin_t,\n",
    "                        wspace = wspace,\n",
    "                        hspace = hspace)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics on grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics_grid(simulator='lotkavolterra',\n",
    "                      method='maf',\n",
    "                      label='',\n",
    "                      reference_method='rascandal',\n",
    "                      reference_label='',\n",
    "                      quantity='log_p_hat',\n",
    "                      n_runs=10,\n",
    "                      reference_n_runs=10,\n",
    "                      expectation_index=312):\n",
    "    \n",
    "    reference_path = '../../data/results/' + simulator + '/' + reference_method + '/'\n",
    "    path = '../../data/results/' + simulator + '/' + method + '/'\n",
    "    sample_label = 'test_zoom_singletheta_evaluated_on_grid_'\n",
    "    \n",
    "    # Reference likelihood\n",
    "    log_p_references = []\n",
    "    t_references = []\n",
    "    for run in range(reference_n_runs):\n",
    "        run_label = '' if run == 0 else '_run' +  str(run)\n",
    "        try:\n",
    "            log_p_reference = np.load(reference_path + quantity + '_' + sample_label + reference_label + run_label + '.npy')\n",
    "            log_p_references.append(\n",
    "                log_p_reference\n",
    "            )\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    log_p_references = np.array(log_p_references)\n",
    "    log_p_reference = np.median(log_p_references, axis=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mses = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        run_label = '' if run == 0 else '_run' +  str(run)\n",
    "        try:\n",
    "            log_p = np.load(\n",
    "                path + quantity + '_' + sample_label + label + run_label + '.npy'\n",
    "            )\n",
    "            log_p = log_p\n",
    "            \n",
    "            n_infs = np.sum(~np.isfinite(log_p))\n",
    "            if n_infs > 0:\n",
    "                print('{} infs for {}'.format(n_infs, method))\n",
    "                log_p[~np.isfinite(log_p)] = -100.\n",
    "            \n",
    "            mses.append(\n",
    "                mean_squared_error(log_p_reference.flatten(), log_p.flatten())\n",
    "            )\n",
    "        \n",
    "        except FileNotFoundError as e:\n",
    "            #print(e)\n",
    "            pass\n",
    "        \n",
    "    mses = np.array(mses)\n",
    "        \n",
    "    # Calculate mean and std\n",
    "    mse_mean = np.median(mses)\n",
    "    mse_uncertainty = np.std(mses) / len(mses)**0.5\n",
    "    \n",
    "    return (mse_mean, mse_uncertainty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "for method, model_label in zip(methods, model_labels):\n",
    "    metrics_this_method = []\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        \n",
    "        samplesize_label = '_trainingsamplesize_' + str(sample_size)\n",
    "            \n",
    "        metrics_this_method.append(\n",
    "            load_metrics_grid(\n",
    "                method=method,\n",
    "                label='_model_zoom' + model_label + samplesize_label,\n",
    "                reference_method='maf',\n",
    "                reference_label='_model_zoom_mog_trainingsamplesize_200000',\n",
    "                quantity='log_p_hat'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    metrics.append(metrics_this_method)\n",
    "    \n",
    "metrics = np.array(metrics)\n",
    "\n",
    "log_p_grid_mse_mean = metrics[:,:,0]\n",
    "log_p_grid_mse_uncertainties = metrics[:,:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/Users/johannbrehmer/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "for method, model_label in zip(methods, model_labels):\n",
    "    metrics_this_method = []\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        \n",
    "        samplesize_label = '_trainingsamplesize_' + str(sample_size)\n",
    "            \n",
    "        metrics_this_method.append(\n",
    "            load_metrics_grid(\n",
    "                method=method,\n",
    "                label='_model_zoom' + model_label + samplesize_label,\n",
    "                reference_method='scandal',\n",
    "                reference_label='_model_zoom_mog_largealpha_trainingsamplesize_200000',\n",
    "                quantity='log_r_hat'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    metrics.append(metrics_this_method)\n",
    "    \n",
    "metrics = np.array(metrics)\n",
    "\n",
    "log_r_grid_mse_mean = metrics[:,:,0]\n",
    "log_r_grid_mse_uncertainties = metrics[:,:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Money plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = custom_figure((1, 2), 4.5, 1.8, reduce_vertical_sep=True)\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[0.3,0.6])\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.subplot(gs[1])\n",
    "\n",
    "for m, method in enumerate(legend_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        log_r_grid_mse_mean[m] - log_r_grid_mse_uncertainties[m],\n",
    "        log_r_grid_mse_mean[m] + log_r_grid_mse_uncertainties[m],\n",
    "        color=colors[m],\n",
    "        alpha=band_alphas[m]\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(legend_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        log_r_grid_mse_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls=linestyles[m],\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=None if method == '' else method\n",
    "    )\n",
    "    \n",
    "plt.legend(loc='lower left', edgecolor='white', facecolor='white', framealpha=0.,\n",
    "           bbox_to_anchor=(0.,-0.01), labelspacing=0.3)\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "plt.ylabel(r'$\\mathrm{MSE} \\; [ \\log \\; \\hat{r} ] $')\n",
    "plt.ylim(0.,0.153)\n",
    "ax.yaxis.set_label_coords(-0.13, 0.5)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(gs[0], sharex=ax)\n",
    "\n",
    "for m, method in enumerate(legend_labels):\n",
    "    if np.any(np.isfinite(log_p_grid_mse_mean[m])):\n",
    "        plt.fill_between(\n",
    "            sample_sizes,\n",
    "            log_p_grid_mse_mean[m] - log_p_grid_mse_uncertainties[m],\n",
    "            log_p_grid_mse_mean[m] + log_p_grid_mse_uncertainties[m],\n",
    "            color=colors[m],\n",
    "            alpha=band_alphas[m]\n",
    "        )\n",
    "    \n",
    "for m, method in enumerate(legend_labels):\n",
    "    if np.any(np.isfinite(log_p_grid_mse_mean[m])):\n",
    "        plt.plot(\n",
    "            sample_sizes,\n",
    "            log_p_grid_mse_mean[m],\n",
    "            color=colors[m],\n",
    "            lw=1.5,\n",
    "            ls=linestyles[m],\n",
    "            marker='o',\n",
    "            ms=5.,\n",
    "            label=None if method == '' else method\n",
    "        )\n",
    "\n",
    "plt.ylabel(r'$\\mathrm{MSE} \\; [ \\log \\; \\hat{p} ] $')\n",
    "ax2.set_yscale(\"log\", nonposy='clip')\n",
    "#plt.ylim(0.,200.)\n",
    "#ax2.xaxis.set_ticks_position(\"none\")\n",
    "plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "ax2.yaxis.set_label_coords(-0.13, 0.5)\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig(\"lotka_volterra_results_checkpoint.pdf\")\n",
    "#plt.savefig(\"lotka_volterra_results.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4.5,calculate_height(1,4.5,1,False)))\n",
    "ax = plt.gca()\n",
    "\n",
    "for m, method in enumerate(legend_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        log_r_grid_mse_mean[m] - log_r_grid_mse_uncertainties[m],\n",
    "        log_r_grid_mse_mean[m] + log_r_grid_mse_uncertainties[m],\n",
    "        color=colors[m],\n",
    "        alpha=band_alphas[m]\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(legend_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        log_r_grid_mse_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=None if method == '' else method\n",
    "    )\n",
    "    \n",
    "plt.legend(loc='lower left', edgecolor='white', facecolor='white', framealpha=1.)\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel(r'$\\mathrm{MSE} [ \\log \\hat{r} ] $')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "plt.ylim(0.,0.13)\n",
    "\n",
    "adjust_margins(1,4.5,1,True)\n",
    "# plt.savefig('lotka_volterra_mse_vs_samplesize.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9.,calculate_height(2,9.,1,False)))\n",
    "\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "\n",
    "for m, method in enumerate(legend_labels):\n",
    "    if np.any(np.isfinite(log_p_grid_mse_mean[m])):\n",
    "        plt.fill_between(\n",
    "            sample_sizes,\n",
    "            log_p_grid_mse_mean[m] - log_p_grid_mse_uncertainties[m],\n",
    "            log_p_grid_mse_mean[m] + log_p_grid_mse_uncertainties[m],\n",
    "            color=colors[m],\n",
    "            alpha=band_alphas[m]\n",
    "        )\n",
    "    \n",
    "for m, method in enumerate(legend_labels):\n",
    "    if np.any(np.isfinite(log_p_grid_mse_mean[m])):\n",
    "        plt.plot(\n",
    "            sample_sizes,\n",
    "            log_p_grid_mse_mean[m],\n",
    "            color=colors[m],\n",
    "            lw=1.5,\n",
    "            ls='-',\n",
    "            marker='o',\n",
    "            ms=5.,\n",
    "            label=method\n",
    "        )\n",
    "    \n",
    "plt.legend(loc='upper right', edgecolor='white', facecolor='white', framealpha=1.)\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel(r'$\\mathrm{MSE} \\; [ \\log \\; \\hat{p} ] $')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "plt.ylim(0.,240.)\n",
    "\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "\n",
    "for m, method in enumerate(legend_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        log_r_grid_mse_mean[m] - log_r_grid_mse_uncertainties[m],\n",
    "        log_r_grid_mse_mean[m] + log_r_grid_mse_uncertainties[m],\n",
    "        color=colors[m],\n",
    "        alpha=band_alphas[m]\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(legend_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        log_r_grid_mse_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=method\n",
    "    )\n",
    "    \n",
    "plt.legend(loc='lower left', edgecolor='white', facecolor='white', framealpha=1.)\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel(r'$\\mathrm{MSE} \\; [ \\log \\; \\hat{r} ] $')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "plt.ylim(0.,0.13)\n",
    "    \n",
    "\n",
    "adjust_margins(2,9.,1,False)\n",
    "# plt.savefig('lotka_volterra_mses_vs_samplesize.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5.,calculate_height(1,5.,1.0,True, True)))\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "for m, method in enumerate(legend_labels):\n",
    "    if np.any(np.isfinite(log_p_grid_mse_mean[m])):\n",
    "        plt.fill_between(\n",
    "            sample_sizes,\n",
    "            log_p_grid_mse_mean[m] - log_p_grid_mse_uncertainties[m],\n",
    "            log_p_grid_mse_mean[m] + log_p_grid_mse_uncertainties[m],\n",
    "            color=colors[m],\n",
    "            alpha=band_alphas[m]\n",
    "        )\n",
    "    \n",
    "for m, method in enumerate(legend_labels):\n",
    "    if np.any(np.isfinite(log_p_grid_mse_mean[m])):\n",
    "        plt.plot(\n",
    "            sample_sizes,\n",
    "            log_p_grid_mse_mean[m],\n",
    "            color=colors[m],\n",
    "            lw=1.5,\n",
    "            ls='--',\n",
    "            marker='s',\n",
    "            ms=5.,\n",
    "            label=method\n",
    "        )\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel(r'Dashed lines: $\\mathrm{MSE} \\; [ \\log \\; \\hat{p} ] $')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "plt.ylim(0.,220.)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = plt.twinx(ax)\n",
    "\n",
    "for m, method in enumerate(legend_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        log_r_grid_mse_mean[m] - log_r_grid_mse_uncertainties[m],\n",
    "        log_r_grid_mse_mean[m] + log_r_grid_mse_uncertainties[m],\n",
    "        color=colors[m],\n",
    "        alpha=band_alphas[m]\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(legend_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        log_r_grid_mse_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=method\n",
    "    )\n",
    "    \n",
    "plt.legend(loc='lower left', edgecolor='white', facecolor='white', framealpha=0.)\n",
    "\n",
    "plt.ylabel(r'Solid lines: $\\mathrm{MSE} \\; [ \\log \\; \\hat{r} ] $')\n",
    "plt.ylim(0.,0.13)\n",
    "    \n",
    "\n",
    "adjust_margins(1,5.,1.0,False, True)\n",
    "#plt.savefig('lotka_volterra_mses_vs_samplesize.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics evaluated on theta used for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(simulator='lotkavolterra',\n",
    "                 method='maf',\n",
    "                 label='',\n",
    "                 reference_method='rascandal',\n",
    "                 reference_label='',\n",
    "                 single_theta=True,\n",
    "                 n_runs=10,\n",
    "                 reference_n_runs=1):\n",
    "    \n",
    "    reference_path = '../../data/results/' + simulator + '/' + reference_method + '/'\n",
    "    path = '../../data/results/' + simulator + '/' + method + '/'\n",
    "    test_label = 'test_zoom_singletheta' if single_theta else 'test_zoom'\n",
    "    \n",
    "    # Reference likelihood\n",
    "    log_p_references = []\n",
    "    t_references = []\n",
    "    for run in range(reference_n_runs):\n",
    "        run_label = '' if run == 0 else '_run' +  str(run)\n",
    "        try:\n",
    "            log_p_references.append(np.load(reference_path + 'log_p_hat_' + test_label + reference_label + run_label + '.npy'))\n",
    "            t_references.append(np.load(reference_path + 't_hat_' + test_label + reference_label + run_label + '.npy'))\n",
    "        except FileNotFoundError as e:\n",
    "            # print(e)\n",
    "            pass\n",
    "    log_p_references = np.array(log_p_references)\n",
    "    log_p_reference = np.median(log_p_references, axis=0)\n",
    "    t_references = np.array(t_references)\n",
    "    t_reference = np.median(t_references, axis=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    expected_log_ps = []\n",
    "    mses = []\n",
    "    score_mses = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        run_label = '' if run == 0 else '_run' +  str(run)\n",
    "        try:\n",
    "            log_p = np.load(\n",
    "                path + 'log_p_hat_' + test_label + label + run_label + '.npy'\n",
    "            )\n",
    "            log_p[~np.isfinite(log_p)] = -100.\n",
    "            \n",
    "            t = np.load(\n",
    "                path + 't_hat_' + test_label + label + run_label + '.npy'\n",
    "            )\n",
    "            t[~np.isfinite(t)] = -100.\n",
    "            \n",
    "            expected_log_ps.append(\n",
    "                1. / log_p.shape[0] * np.sum(log_p)\n",
    "            )\n",
    "            mses.append(\n",
    "                mean_squared_error(log_p_reference, log_p)\n",
    "            )\n",
    "            score_mses.append(\n",
    "                mean_squared_error(t_reference, t)\n",
    "            )\n",
    "        \n",
    "        except FileNotFoundError as e:\n",
    "            # print(e)\n",
    "            pass\n",
    "        \n",
    "    expected_log_ps = np.array(expected_log_ps)\n",
    "    mses = np.array(mses)\n",
    "    score_mses = np.array(score_mses)\n",
    "        \n",
    "    # Calculate mean and std\n",
    "    expected_log_p_mean = np.mean(expected_log_ps)\n",
    "    expected_log_p_uncertainty = np.std(expected_log_ps) / len(expected_log_ps)**0.5\n",
    "    \n",
    "    mse_mean = np.mean(mses)\n",
    "    mse_uncertainty = np.std(mses) / len(mses)**0.5\n",
    "    \n",
    "    score_mse_mean = np.mean(score_mses)\n",
    "    score_mse_uncertainty = np.std(score_mses) / len(score_mses)**0.5\n",
    "    \n",
    "    return (expected_log_p_mean, expected_log_p_uncertainty,\n",
    "            mse_mean, mse_uncertainty,\n",
    "            score_mse_mean, score_mse_uncertainty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [1000, 2000, 5000, 10000, 20000, 50000, 100000]\n",
    "\n",
    "methods = ['maf', 'maf', 'scandal', 'scandal', 'scandal', 'scandal', 'rascandal', 'rascandal']\n",
    "filenames = ['', '_mog', '', '_largealpha', '_mog', '_mog_largealpha', '', '_mog']\n",
    "method_labels = ['MAF', 'MAF MoG', 'SCANDAL', r'SCANDAL (large $\\alpha$)', 'SCANDAL MoG', r'SCANDAL MoG(large $\\alpha$)', 'RASCANDAL', 'RASCANDAL MoG']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for method, filename in zip(methods[:], filenames[:]):\n",
    "    metrics_this_method = []\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        \n",
    "        samplesize_label = '_trainingsamplesize_' + str(sample_size)\n",
    "            \n",
    "        metrics_this_method.append(\n",
    "            load_metrics(\n",
    "                method=method,\n",
    "                label='_model_zoom' +  filename + samplesize_label,\n",
    "                reference_method='maf',\n",
    "                reference_label='_model_zoom_mog_trainingsamplesize_100000',\n",
    "                single_theta=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    metrics.append(metrics_this_method)\n",
    "    \n",
    "metrics = np.array(metrics)\n",
    "\n",
    "expected_log_p_mean = metrics[:,:,0]\n",
    "expected_log_p_uncertainty = metrics[:,:,1]\n",
    "mse_mean = metrics[:,:,2]\n",
    "mse_uncertainty = metrics[:,:,3]\n",
    "score_mse_mean = metrics[:,:,4]\n",
    "score_mse_uncertainty = metrics[:,:,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for method, filename in zip(methods[:], filenames[:]):\n",
    "    metrics_this_method = []\n",
    "    \n",
    "    for sample_size in sample_sizes:\n",
    "        \n",
    "        samplesize_label = '_trainingsamplesize_' + str(sample_size)\n",
    "            \n",
    "        metrics_this_method.append(\n",
    "            load_metrics(\n",
    "                method=method,\n",
    "                label='_model_zoom' +  filename + samplesize_label,\n",
    "                reference_method='maf',\n",
    "                reference_label='_model_zoom_mog_trainingsamplesize_100000',\n",
    "                single_theta=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    metrics.append(metrics_this_method)\n",
    "    \n",
    "metrics = np.array(metrics)\n",
    "\n",
    "expected_log_p_st_mean = metrics[:,:,0]\n",
    "expected_log_p_st_uncertainty = metrics[:,:,1]\n",
    "mse_st_mean = metrics[:,:,2]\n",
    "mse_st_uncertainty = metrics[:,:,3]\n",
    "score_mse_st_mean = metrics[:,:,4]\n",
    "score_mse_st_uncertainty = metrics[:,:,5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['C' + str(i) for i in range(10)]\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.subplot(2,3,1)\n",
    "\n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        - expected_log_p_st_mean[m] - expected_log_p_st_uncertainty[m],\n",
    "        - expected_log_p_st_mean[m] + expected_log_p_st_uncertainty[m],\n",
    "        color=colors[m],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        - expected_log_p_st_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=method\n",
    "    )\n",
    "    \n",
    "#plt.legend()\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel('Negative log likelihood')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "#plt.ylim(-20.,200.)\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.subplot(2,3,2)\n",
    "\n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        mse_st_mean[m] - mse_st_uncertainty[m],\n",
    "        mse_st_mean[m] + mse_st_uncertainty[m],\n",
    "        color=colors[m],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        mse_st_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=method\n",
    "    )\n",
    "    \n",
    "#plt.legend()\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel('MSE (log likelihood) wrt high-statistics model')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.ylim(1.,2000.)\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.subplot(2,3,3)\n",
    "\n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        score_mse_st_mean[m] - score_mse_st_uncertainty[m],\n",
    "        score_mse_st_mean[m] + score_mse_st_uncertainty[m],\n",
    "        color=colors[m],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        score_mse_st_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=method\n",
    "    )\n",
    "    \n",
    "#plt.legend()\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel('MSE (score) wrt high-statistics model')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "#ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.ylim(0.,None)\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.subplot(2,3,4)\n",
    "\n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        - expected_log_p_mean[m] - expected_log_p_uncertainty[m],\n",
    "        - expected_log_p_mean[m] + expected_log_p_uncertainty[m],\n",
    "        color=colors[m],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        - expected_log_p_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=method\n",
    "    )\n",
    "    \n",
    "#plt.legend()\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel('Negative log likelihood')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "#plt.ylim(-20.,200.)\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.subplot(2,3,5)\n",
    "\n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        mse_mean[m] - mse_uncertainty[m],\n",
    "        mse_mean[m] + mse_uncertainty[m],\n",
    "        color=colors[m],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        mse_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=method\n",
    "    )\n",
    "    \n",
    "#plt.legend()\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel('MSE (log likelihood) wrt high-statistics model')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.ylim(1.,2000.)\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.subplot(2,3,6)\n",
    "\n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.fill_between(\n",
    "        sample_sizes,\n",
    "        score_mse_mean[m] - score_mse_uncertainty[m],\n",
    "        score_mse_mean[m] + score_mse_uncertainty[m],\n",
    "        color=colors[m],\n",
    "        alpha=0.3\n",
    "    )\n",
    "    \n",
    "for m, method in enumerate(method_labels):\n",
    "    plt.plot(\n",
    "        sample_sizes,\n",
    "        score_mse_mean[m],\n",
    "        color=colors[m],\n",
    "        lw=1.5,\n",
    "        ls='-',\n",
    "        marker='o',\n",
    "        ms=5.,\n",
    "        label=method\n",
    "    )\n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Training sample size')\n",
    "plt.ylabel('MSE (score) wrt high-statistics model')\n",
    "ax.set_xscale(\"log\", nonposx='clip')\n",
    "#ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.ylim(0.,None)\n",
    "    \n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
